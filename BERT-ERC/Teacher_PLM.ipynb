{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b8e8ed2662e43698eddb168fed62dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3a55c23c71241c1bfae7c065938f057",
              "IPY_MODEL_2231ed82560e4388986a3b91810d9711",
              "IPY_MODEL_55c473372bfb4ddc93135be044cf7237"
            ],
            "layout": "IPY_MODEL_2bdd59d362ce44259b24c902c5f82319"
          }
        },
        "c3a55c23c71241c1bfae7c065938f057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06cff95f851c48d69538b9df1a94f016",
            "placeholder": "​",
            "style": "IPY_MODEL_e98ec2d36eee44f1866b30bb5fa4125d",
            "value": "Map: 100%"
          }
        },
        "2231ed82560e4388986a3b91810d9711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dba6779e4734ac098275c74edc48659",
            "max": 9988,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8f59e249e6044e79c46a04d891e6e58",
            "value": 9988
          }
        },
        "55c473372bfb4ddc93135be044cf7237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaf4a8a8ebe04eb4956afef90aa83121",
            "placeholder": "​",
            "style": "IPY_MODEL_d9bbd9bb151f4d0fb56893396b155ee8",
            "value": " 9988/9988 [00:01&lt;00:00, 4981.71 examples/s]"
          }
        },
        "2bdd59d362ce44259b24c902c5f82319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06cff95f851c48d69538b9df1a94f016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98ec2d36eee44f1866b30bb5fa4125d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dba6779e4734ac098275c74edc48659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f59e249e6044e79c46a04d891e6e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaf4a8a8ebe04eb4956afef90aa83121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9bbd9bb151f4d0fb56893396b155ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b41593993064db6acb7a3595138a9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92883e84f25b4cfcb9d8b95130e20281",
              "IPY_MODEL_1d5a198a2962482d890958a4c194ceb5",
              "IPY_MODEL_57596d470c7e4e3a90631fb58759f673"
            ],
            "layout": "IPY_MODEL_5b4d87592cde43e89d0f2343bdb647c3"
          }
        },
        "92883e84f25b4cfcb9d8b95130e20281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_845467afd7824e03acd13fc7fe74c76c",
            "placeholder": "​",
            "style": "IPY_MODEL_81869faa0c03443a8d9e297b4559df2d",
            "value": "Map: 100%"
          }
        },
        "1d5a198a2962482d890958a4c194ceb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36b48c8d762a47dd8c8dd39c78e48a31",
            "max": 1108,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a9c801d719b4df79b01ed8ea63b11ec",
            "value": 1108
          }
        },
        "57596d470c7e4e3a90631fb58759f673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a9a8d00aa8b4162abe46061bb91722e",
            "placeholder": "​",
            "style": "IPY_MODEL_c0e7104b983f4250aec6104b1f25a1e4",
            "value": " 1108/1108 [00:00&lt;00:00, 7908.50 examples/s]"
          }
        },
        "5b4d87592cde43e89d0f2343bdb647c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "845467afd7824e03acd13fc7fe74c76c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81869faa0c03443a8d9e297b4559df2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36b48c8d762a47dd8c8dd39c78e48a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a9c801d719b4df79b01ed8ea63b11ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a9a8d00aa8b4162abe46061bb91722e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0e7104b983f4250aec6104b1f25a1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68ec54082c7440c4a5341b6948a43b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1eb76692bfb140af89093adbfa042a33",
              "IPY_MODEL_046cc1bc39964a2ab099cdb16e83a0f0",
              "IPY_MODEL_d89014565d144daf9e6f70576ba8d004"
            ],
            "layout": "IPY_MODEL_4f143167cc5943ca82d8eebdb626fc78"
          }
        },
        "1eb76692bfb140af89093adbfa042a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba952df26e44c05b3249f196abc839f",
            "placeholder": "​",
            "style": "IPY_MODEL_fa7d98f0ec8d42579a0b88ae8974c598",
            "value": "Map: 100%"
          }
        },
        "046cc1bc39964a2ab099cdb16e83a0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9435936217c64e98b8504b194c3a3391",
            "max": 2610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b04baea92d243bcafc035ece8de86c7",
            "value": 2610
          }
        },
        "d89014565d144daf9e6f70576ba8d004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17e365db8524492983959ee37c50b309",
            "placeholder": "​",
            "style": "IPY_MODEL_bc3114d7e4a346f2908a56a13af7e89d",
            "value": " 2610/2610 [00:00&lt;00:00, 8239.39 examples/s]"
          }
        },
        "4f143167cc5943ca82d8eebdb626fc78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba952df26e44c05b3249f196abc839f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7d98f0ec8d42579a0b88ae8974c598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9435936217c64e98b8504b194c3a3391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b04baea92d243bcafc035ece8de86c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17e365db8524492983959ee37c50b309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc3114d7e4a346f2908a56a13af7e89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9853e646166a452c843e0d935503e9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_185d571e712541168a37890dfbc5182d",
              "IPY_MODEL_80a9ffead2094ecab21a67d2f0a08bd6",
              "IPY_MODEL_42815d8844a6451883113be09b4aa472"
            ],
            "layout": "IPY_MODEL_d3f033d74b314ad981b12683a1077780"
          }
        },
        "185d571e712541168a37890dfbc5182d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df8fddebf9854a3d8527714a8433f71b",
            "placeholder": "​",
            "style": "IPY_MODEL_5eb7e40e420548b8a9787c692090cc57",
            "value": "Map: 100%"
          }
        },
        "80a9ffead2094ecab21a67d2f0a08bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a31f21eb38847d093f89818c179237c",
            "max": 9988,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94dad761462246d1afdc0e3de92f682e",
            "value": 9988
          }
        },
        "42815d8844a6451883113be09b4aa472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47da2bb7059549e4972922dcc6c23064",
            "placeholder": "​",
            "style": "IPY_MODEL_18a01e7e7f694d68a800849236a69bf8",
            "value": " 9988/9988 [00:00&lt;00:00, 12684.95 examples/s]"
          }
        },
        "d3f033d74b314ad981b12683a1077780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df8fddebf9854a3d8527714a8433f71b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eb7e40e420548b8a9787c692090cc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a31f21eb38847d093f89818c179237c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94dad761462246d1afdc0e3de92f682e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47da2bb7059549e4972922dcc6c23064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a01e7e7f694d68a800849236a69bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d4d6b68b19f464ba05b3982f436c035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51674f029bd94ff8aba443f00c1217a1",
              "IPY_MODEL_ffd523b6ecf2496b92bd7775c4135013",
              "IPY_MODEL_bc33ba753a024bf0aaafca8397ca2ff5"
            ],
            "layout": "IPY_MODEL_80451ece4c494be68fc70912892168f2"
          }
        },
        "51674f029bd94ff8aba443f00c1217a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87656ae318349d4a34bb531456f7576",
            "placeholder": "​",
            "style": "IPY_MODEL_f5a9023d00094eb1b707c8760cfc2a51",
            "value": "Map: 100%"
          }
        },
        "ffd523b6ecf2496b92bd7775c4135013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f2db93a673d48b4b1e7bb14a760056b",
            "max": 1108,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1375b3b39077459086391f9df4ed7af9",
            "value": 1108
          }
        },
        "bc33ba753a024bf0aaafca8397ca2ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1475ac3827ad43498478c56ff63388e8",
            "placeholder": "​",
            "style": "IPY_MODEL_462c653134a34857871987df5a2f3aea",
            "value": " 1108/1108 [00:00&lt;00:00, 10963.97 examples/s]"
          }
        },
        "80451ece4c494be68fc70912892168f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f87656ae318349d4a34bb531456f7576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a9023d00094eb1b707c8760cfc2a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f2db93a673d48b4b1e7bb14a760056b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1375b3b39077459086391f9df4ed7af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1475ac3827ad43498478c56ff63388e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "462c653134a34857871987df5a2f3aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc9b9282386a402382b7bd815ebbe697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5eac480bd0847f792e10414c502d8be",
              "IPY_MODEL_c88a5d9e0a9f4cf2ad440b966d7c88a4",
              "IPY_MODEL_28e4ae6cbcf3454ab1cdd3e8ff5905e0"
            ],
            "layout": "IPY_MODEL_efff791161284a4fa87925578f9257e6"
          }
        },
        "e5eac480bd0847f792e10414c502d8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a23338fb1de44d64a187593880c0ddf7",
            "placeholder": "​",
            "style": "IPY_MODEL_e19022c09395481b974f9e926853d7b9",
            "value": "Map: 100%"
          }
        },
        "c88a5d9e0a9f4cf2ad440b966d7c88a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d44115c4439c48d9ac0a163d97e7ae6e",
            "max": 2610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9b2fdabc1a047f1845f63d35f97140e",
            "value": 2610
          }
        },
        "28e4ae6cbcf3454ab1cdd3e8ff5905e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f384a5e548c54f66a8667d097132284b",
            "placeholder": "​",
            "style": "IPY_MODEL_562f0161a344447e8a98ad0796a730b1",
            "value": " 2610/2610 [00:00&lt;00:00, 12205.41 examples/s]"
          }
        },
        "efff791161284a4fa87925578f9257e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a23338fb1de44d64a187593880c0ddf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e19022c09395481b974f9e926853d7b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d44115c4439c48d9ac0a163d97e7ae6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b2fdabc1a047f1845f63d35f97140e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f384a5e548c54f66a8667d097132284b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562f0161a344447e8a98ad0796a730b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XzZ7o0Etimd",
        "outputId": "dc37578d-e8d3-45b6-ceb8-fbc6554d69c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728,
          "referenced_widgets": [
            "0b8e8ed2662e43698eddb168fed62dcd",
            "c3a55c23c71241c1bfae7c065938f057",
            "2231ed82560e4388986a3b91810d9711",
            "55c473372bfb4ddc93135be044cf7237",
            "2bdd59d362ce44259b24c902c5f82319",
            "06cff95f851c48d69538b9df1a94f016",
            "e98ec2d36eee44f1866b30bb5fa4125d",
            "6dba6779e4734ac098275c74edc48659",
            "a8f59e249e6044e79c46a04d891e6e58",
            "aaf4a8a8ebe04eb4956afef90aa83121",
            "d9bbd9bb151f4d0fb56893396b155ee8",
            "8b41593993064db6acb7a3595138a9e8",
            "92883e84f25b4cfcb9d8b95130e20281",
            "1d5a198a2962482d890958a4c194ceb5",
            "57596d470c7e4e3a90631fb58759f673",
            "5b4d87592cde43e89d0f2343bdb647c3",
            "845467afd7824e03acd13fc7fe74c76c",
            "81869faa0c03443a8d9e297b4559df2d",
            "36b48c8d762a47dd8c8dd39c78e48a31",
            "0a9c801d719b4df79b01ed8ea63b11ec",
            "2a9a8d00aa8b4162abe46061bb91722e",
            "c0e7104b983f4250aec6104b1f25a1e4",
            "68ec54082c7440c4a5341b6948a43b77",
            "1eb76692bfb140af89093adbfa042a33",
            "046cc1bc39964a2ab099cdb16e83a0f0",
            "d89014565d144daf9e6f70576ba8d004",
            "4f143167cc5943ca82d8eebdb626fc78",
            "4ba952df26e44c05b3249f196abc839f",
            "fa7d98f0ec8d42579a0b88ae8974c598",
            "9435936217c64e98b8504b194c3a3391",
            "7b04baea92d243bcafc035ece8de86c7",
            "17e365db8524492983959ee37c50b309",
            "bc3114d7e4a346f2908a56a13af7e89d",
            "9853e646166a452c843e0d935503e9eb",
            "185d571e712541168a37890dfbc5182d",
            "80a9ffead2094ecab21a67d2f0a08bd6",
            "42815d8844a6451883113be09b4aa472",
            "d3f033d74b314ad981b12683a1077780",
            "df8fddebf9854a3d8527714a8433f71b",
            "5eb7e40e420548b8a9787c692090cc57",
            "8a31f21eb38847d093f89818c179237c",
            "94dad761462246d1afdc0e3de92f682e",
            "47da2bb7059549e4972922dcc6c23064",
            "18a01e7e7f694d68a800849236a69bf8",
            "2d4d6b68b19f464ba05b3982f436c035",
            "51674f029bd94ff8aba443f00c1217a1",
            "ffd523b6ecf2496b92bd7775c4135013",
            "bc33ba753a024bf0aaafca8397ca2ff5",
            "80451ece4c494be68fc70912892168f2",
            "f87656ae318349d4a34bb531456f7576",
            "f5a9023d00094eb1b707c8760cfc2a51",
            "7f2db93a673d48b4b1e7bb14a760056b",
            "1375b3b39077459086391f9df4ed7af9",
            "1475ac3827ad43498478c56ff63388e8",
            "462c653134a34857871987df5a2f3aea",
            "dc9b9282386a402382b7bd815ebbe697",
            "e5eac480bd0847f792e10414c502d8be",
            "c88a5d9e0a9f4cf2ad440b966d7c88a4",
            "28e4ae6cbcf3454ab1cdd3e8ff5905e0",
            "efff791161284a4fa87925578f9257e6",
            "a23338fb1de44d64a187593880c0ddf7",
            "e19022c09395481b974f9e926853d7b9",
            "d44115c4439c48d9ac0a163d97e7ae6e",
            "d9b2fdabc1a047f1845f63d35f97140e",
            "f384a5e548c54f66a8667d097132284b",
            "562f0161a344447e8a98ad0796a730b1"
          ]
        },
        "id": "tVf-go68mV_u",
        "outputId": "3ad3aba2-f880-46f1-814e-c34936273730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9988 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b8e8ed2662e43698eddb168fed62dcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1108 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b41593993064db6acb7a3595138a9e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2610 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68ec54082c7440c4a5341b6948a43b77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9988 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9853e646166a452c843e0d935503e9eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1108 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d4d6b68b19f464ba05b3982f436c035"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2610 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc9b9282386a402382b7bd815ebbe697"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-6-a141680e9678>:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkonthodores\u001b[0m (\u001b[33mkonthodores-university-of-patras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250528_165953-m8u1uwi9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/konthodores-university-of-patras/huggingface/runs/m8u1uwi9' target=\"_blank\">/content/drive/MyDrive/MELD/teacher_roberta_erc</a></strong> to <a href='https://wandb.ai/konthodores-university-of-patras/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/konthodores-university-of-patras/huggingface' target=\"_blank\">https://wandb.ai/konthodores-university-of-patras/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/konthodores-university-of-patras/huggingface/runs/m8u1uwi9' target=\"_blank\">https://wandb.ai/konthodores-university-of-patras/huggingface/runs/m8u1uwi9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1875/1875 06:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.244600</td>\n",
              "      <td>1.201751</td>\n",
              "      <td>0.612816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.030200</td>\n",
              "      <td>1.139270</td>\n",
              "      <td>0.629061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.901000</td>\n",
              "      <td>1.133685</td>\n",
              "      <td>0.620036</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "from google.colab import drive\n",
        "import os\n",
        "import evaluate\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ─── Configuration ────────────────────────────────────────────────────────────\n",
        "MODEL_CHECKPOINT = \"roberta-base\"  # public alias for Facebook's RoBERTa\n",
        "EMOTIONS        = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
        "NUM_LABELS      = len(EMOTIONS)\n",
        "MAX_LEN         = 128\n",
        "BATCH_SIZE      = 16\n",
        "EPOCHS          = 3\n",
        "LR              = 2e-5\n",
        "\n",
        "# ─── 1) Load CSVs into pandas ─────────────────────────────────────────────────\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/MELD/train_with_context.csv\")\n",
        "df_dev   = pd.read_csv(\"/content/drive/MyDrive/MELD/dev_with_context.csv\")\n",
        "df_test  = pd.read_csv(\"/content/drive/MyDrive/MELD/test_with_context.csv\")\n",
        "\n",
        "# ─── 2) Convert pandas DataFrames to Hugging Face Datasets ────────────────────\n",
        "train_ds = Dataset.from_pandas(df_train)\n",
        "dev_ds   = Dataset.from_pandas(df_dev)\n",
        "test_ds  = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Remove the pandas index column if present\n",
        "for ds in (train_ds, dev_ds, test_ds):\n",
        "    if \"_pandas_index\" in ds.column_names:\n",
        "        ds = ds.remove_columns([\"_pandas_index\"])\n",
        "\n",
        "dataset = DatasetDict({\"train\": train_ds, \"validation\": dev_ds, \"test\": test_ds})\n",
        "\n",
        "# ─── 3) Encode labels as ClassLabel ───────────────────────────────────────────\n",
        "label_feature = ClassLabel(names=EMOTIONS)\n",
        "\n",
        "def add_labels(example):\n",
        "    example[\"labels\"] = label_feature.str2int(example[\"Emotion\"])\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_labels)\n",
        "\n",
        "# ─── 4) Tokenize bert_input column ────────────────────────────────────────────\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def tokenize_batch(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"bert_input\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(tokenize_batch, batched=True)\n",
        "\n",
        "# ─── 5) Prepare for PyTorch ───────────────────────────────────────────────────\n",
        "dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n",
        "\n",
        "# ─── 6) Load the RoBERTa sequence classification model ────────────────────────\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=NUM_LABELS\n",
        ")\n",
        "\n",
        "# ─── 7) Define evaluation metric ──────────────────────────────────────────────\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "# ─── 8) Set up training arguments ──────────────────────────────────────────────\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/MELD/teacher_roberta_erc\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        ")\n",
        "\n",
        "# ─── 9) Initialize Trainer and fine-tune ─────────────────────────────────────\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"/content/drive/MyDrive/MELD/teacher_roberta_erc\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "import inspect\n",
        "\n",
        "# Print the signature\n",
        "print(inspect.signature(TrainingArguments.__init__))\n",
        "\n",
        "# Or get full help text\n",
        "help(TrainingArguments)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XpOf1Ktz_9T",
        "outputId": "03a05c11-3305-4b42-ccaf-49681ea42b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict, str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict, str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n",
            "Help on class TrainingArguments in module transformers.training_args:\n",
            "\n",
            "class TrainingArguments(builtins.object)\n",
            " |  TrainingArguments(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict, str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict, str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n",
            " |  \n",
            " |  TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
            " |  itself**.\n",
            " |  \n",
            " |  Using [`HfArgumentParser`] we can turn this class into\n",
            " |  [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
            " |  command line.\n",
            " |  \n",
            " |  Parameters:\n",
            " |      output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n",
            " |          The output directory where the model predictions and checkpoints will be written.\n",
            " |      overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
            " |          If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
            " |          points to a checkpoint directory.\n",
            " |      do_train (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
            " |          by your training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      do_eval (`bool`, *optional*):\n",
            " |          Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is\n",
            " |          different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n",
            " |          training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      do_predict (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n",
            " |          intended to be used by your training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
            " |          The evaluation strategy to adopt during training. Possible values are:\n",
            " |  \n",
            " |              - `\"no\"`: No evaluation is done during training.\n",
            " |              - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
            " |              - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
            " |  \n",
            " |      prediction_loss_only (`bool`, *optional*, defaults to `False`):\n",
            " |          When performing evaluation and generating predictions, only returns the loss.\n",
            " |      per_device_train_batch_size (`int`, *optional*, defaults to 8):\n",
            " |          The batch size per device accelerator core/CPU for training.\n",
            " |      per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n",
            " |          The batch size per device accelerator core/CPU for evaluation.\n",
            " |      gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
            " |          Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
            " |  \n",
            " |          <Tip warning={true}>\n",
            " |  \n",
            " |          When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n",
            " |          evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n",
            " |  \n",
            " |          </Tip>\n",
            " |  \n",
            " |      eval_accumulation_steps (`int`, *optional*):\n",
            " |          Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
            " |          left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\n",
            " |          requires more memory).\n",
            " |      eval_delay (`float`, *optional*):\n",
            " |          Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
            " |          eval_strategy.\n",
            " |      torch_empty_cache_steps (`int`, *optional*):\n",
            " |          Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.\n",
            " |  \n",
            " |          <Tip>\n",
            " |  \n",
            " |          This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).\n",
            " |  \n",
            " |          </Tip>\n",
            " |  \n",
            " |      learning_rate (`float`, *optional*, defaults to 5e-5):\n",
            " |          The initial learning rate for [`AdamW`] optimizer.\n",
            " |      weight_decay (`float`, *optional*, defaults to 0):\n",
            " |          The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
            " |          optimizer.\n",
            " |      adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
            " |          The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
            " |      adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
            " |          The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
            " |      adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
            " |          The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
            " |      max_grad_norm (`float`, *optional*, defaults to 1.0):\n",
            " |          Maximum gradient norm (for gradient clipping).\n",
            " |      num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
            " |          Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
            " |          the last epoch before stopping training).\n",
            " |      max_steps (`int`, *optional*, defaults to -1):\n",
            " |          If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
            " |          For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
            " |          `max_steps` is reached.\n",
            " |      lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
            " |          The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
            " |      lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n",
            " |          The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n",
            " |      warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
            " |          Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
            " |      warmup_steps (`int`, *optional*, defaults to 0):\n",
            " |          Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
            " |      log_level (`str`, *optional*, defaults to `passive`):\n",
            " |          Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
            " |          'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n",
            " |          current log level for the Transformers library (which will be `\"warning\"` by default).\n",
            " |      log_level_replica (`str`, *optional*, defaults to `\"warning\"`):\n",
            " |          Logger log level to use on replicas. Same choices as `log_level`\"\n",
            " |      log_on_each_node (`bool`, *optional*, defaults to `True`):\n",
            " |          In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
            " |          node.\n",
            " |      logging_dir (`str`, *optional*):\n",
            " |          [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
            " |          *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
            " |      logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |          The logging strategy to adopt during training. Possible values are:\n",
            " |  \n",
            " |              - `\"no\"`: No logging is done during training.\n",
            " |              - `\"epoch\"`: Logging is done at the end of each epoch.\n",
            " |              - `\"steps\"`: Logging is done every `logging_steps`.\n",
            " |  \n",
            " |      logging_first_step (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to log the first `global_step` or not.\n",
            " |      logging_steps (`int` or `float`, *optional*, defaults to 500):\n",
            " |          Number of update steps between two logs if `logging_strategy=\"steps\"`. Should be an integer or a float in\n",
            " |          range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
            " |      logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n",
            " |          or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
            " |  \n",
            " |          <Tip>\n",
            " |  \n",
            " |          `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
            " |          gradient is computed or applied to the model.\n",
            " |  \n",
            " |          </Tip>\n",
            " |  \n",
            " |      save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |          The checkpoint save strategy to adopt during training. Possible values are:\n",
            " |  \n",
            " |              - `\"no\"`: No save is done during training.\n",
            " |              - `\"epoch\"`: Save is done at the end of each epoch.\n",
            " |              - `\"steps\"`: Save is done every `save_steps`.\n",
            " |              - `\"best\"`: Save is done whenever a new `best_metric` is achieved.\n",
            " |  \n",
            " |              If `\"epoch\"` or `\"steps\"` is chosen, saving will also be performed at the\n",
            " |              very end of training, always.\n",
            " |      save_steps (`int` or `float`, *optional*, defaults to 500):\n",
            " |          Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`. Should be an integer or a\n",
            " |          float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
            " |      save_total_limit (`int`, *optional*):\n",
            " |          If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
            " |          `output_dir`. When `load_best_model_at_end` is enabled, the \"best\" checkpoint according to\n",
            " |          `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for\n",
            " |          `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained\n",
            " |          alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two\n",
            " |          checkpoints are saved: the last one and the best one (if they are different).\n",
            " |      save_safetensors (`bool`, *optional*, defaults to `True`):\n",
            " |          Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n",
            " |          default `torch.load` and `torch.save`.\n",
            " |      save_on_each_node (`bool`, *optional*, defaults to `False`):\n",
            " |          When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
            " |          the main one.\n",
            " |  \n",
            " |          This should not be activated when the different nodes use the same storage as the files will be saved with\n",
            " |          the same names for each node.\n",
            " |      save_only_model (`bool`, *optional*, defaults to `False`):\n",
            " |          When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n",
            " |          Note that when this is true, you won't be able to resume training from checkpoint.\n",
            " |          This enables you to save storage by not storing the optimizer, scheduler & rng state.\n",
            " |          You can only load the model using `from_pretrained` with this option set to `True`.\n",
            " |      restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to restore the callback states from the checkpoint. If `True`, will override\n",
            " |          callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n",
            " |      use_cpu (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n",
            " |      seed (`int`, *optional*, defaults to 42):\n",
            " |          Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
            " |          [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n",
            " |      data_seed (`int`, *optional*):\n",
            " |          Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
            " |          same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n",
            " |          seed.\n",
            " |      jit_mode_eval (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to use PyTorch jit trace for inference.\n",
            " |      use_ipex (`bool`, *optional*, defaults to `False`):\n",
            " |          Use Intel extension for PyTorch when it is available. [IPEX\n",
            " |          installation](https://github.com/intel/intel-extension-for-pytorch).\n",
            " |      bf16 (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
            " |          NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n",
            " |      fp16 (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
            " |      fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n",
            " |          For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n",
            " |          the [Apex documentation](https://nvidia.github.io/apex/amp).\n",
            " |      fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
            " |          This argument is deprecated. Use `half_precision_backend` instead.\n",
            " |      half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
            " |          The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n",
            " |          use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n",
            " |          requested backend.\n",
            " |      bf16_full_eval (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
            " |          metric values. This is an experimental API and it may change.\n",
            " |      fp16_full_eval (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
            " |          metric values.\n",
            " |      tf32 (`bool`, *optional*):\n",
            " |          Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n",
            " |          on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n",
            " |          the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n",
            " |          experimental API and it may change.\n",
            " |      local_rank (`int`, *optional*, defaults to -1):\n",
            " |          Rank of the process during distributed training.\n",
            " |      ddp_backend (`str`, *optional*):\n",
            " |          The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n",
            " |      tpu_num_cores (`int`, *optional*):\n",
            " |          When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
            " |      dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
            " |          or not.\n",
            " |      eval_steps (`int` or `float`, *optional*):\n",
            " |          Number of update steps between two evaluations if `eval_strategy=\"steps\"`. Will default to the same\n",
            " |          value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,\n",
            " |          will be interpreted as ratio of total training steps.\n",
            " |      dataloader_num_workers (`int`, *optional*, defaults to 0):\n",
            " |          Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
            " |          main process.\n",
            " |      past_index (`int`, *optional*, defaults to -1):\n",
            " |          Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n",
            " |          the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n",
            " |          use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n",
            " |          training step under the keyword argument `mems`.\n",
            " |      run_name (`str`, *optional*, defaults to `output_dir`):\n",
            " |          A descriptor for the run. Typically used for [wandb](https://www.wandb.com/),\n",
            " |          [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and [swanlab](https://swanlab.cn)\n",
            " |          logging. If not specified, will be the same as `output_dir`.\n",
            " |      disable_tqdm (`bool`, *optional*):\n",
            " |          Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
            " |          [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n",
            " |          set to warn or lower (default), `False` otherwise.\n",
            " |      remove_unused_columns (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether or not to automatically remove the columns unused by the model forward method.\n",
            " |      label_names (`List[str]`, *optional*):\n",
            " |          The list of keys in your dictionary of inputs that correspond to the labels.\n",
            " |  \n",
            " |          Will eventually default to the list of argument names accepted by the model that contain the word \"label\",\n",
            " |          except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the\n",
            " |          `[\"start_positions\", \"end_positions\"]` keys.\n",
            " |      load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to load the best model found during training at the end of training. When this option is\n",
            " |          enabled, the best checkpoint will always be saved. See\n",
            " |          [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)\n",
            " |          for more.\n",
            " |  \n",
            " |          <Tip>\n",
            " |  \n",
            " |          When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in\n",
            " |          the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
            " |  \n",
            " |          </Tip>\n",
            " |  \n",
            " |      metric_for_best_model (`str`, *optional*):\n",
            " |          Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n",
            " |          models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`.\n",
            " |  \n",
            " |          If not specified, this will default to `\"loss\"` when either `load_best_model_at_end == True`\n",
            " |          or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).\n",
            " |  \n",
            " |          If you set this value, `greater_is_better` will default to `True` unless the name ends with \"loss\".\n",
            " |          Don't forget to set it to `False` if your metric is better when lower.\n",
            " |      greater_is_better (`bool`, *optional*):\n",
            " |          Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n",
            " |          should have a greater metric or not. Will default to:\n",
            " |  \n",
            " |          - `True` if `metric_for_best_model` is set to a value that doesn't end in `\"loss\"`.\n",
            " |          - `False` if `metric_for_best_model` is not set, or set to a value that ends in `\"loss\"`.\n",
            " |      ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
            " |          When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
            " |          stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n",
            " |          can take a long time) but will not yield the same results as the interrupted training would have.\n",
            " |      fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `''`):\n",
            " |          Use PyTorch Distributed Parallel Training (in distributed training only).\n",
            " |  \n",
            " |          A list of options along the following:\n",
            " |  \n",
            " |          - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n",
            " |          - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n",
            " |          - `\"hybrid_shard\"`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.\n",
            " |          - `\"hybrid_shard_zero2\"`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.\n",
            " |          - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n",
            " |            `\"shard_grad_op\"`).\n",
            " |          - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n",
            " |      fsdp_config (`str` or `dict`, *optional*):\n",
            " |          Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\n",
            " |          fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n",
            " |  \n",
            " |          A List of config and its options:\n",
            " |              - min_num_params (`int`, *optional*, defaults to `0`):\n",
            " |                  FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n",
            " |                  passed).\n",
            " |              - transformer_layer_cls_to_wrap (`List[str]`, *optional*):\n",
            " |                  List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,\n",
            " |                  `T5Block` .... (useful only when `fsdp` flag is passed).\n",
            " |              - backward_prefetch (`str`, *optional*)\n",
            " |                  FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n",
            " |                  `fsdp` field is passed).\n",
            " |  \n",
            " |                  A list of options along the following:\n",
            " |  \n",
            " |                  - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n",
            " |                    gradient\n",
            " |                      computation.\n",
            " |                  - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n",
            " |                    parameter’s\n",
            " |                      gradient computation.\n",
            " |              - forward_prefetch (`bool`, *optional*, defaults to `False`)\n",
            " |                  FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n",
            " |                   If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n",
            " |                   forward pass.\n",
            " |              - limit_all_gathers (`bool`, *optional*, defaults to `False`)\n",
            " |                  FSDP's limit_all_gathers (useful only when `fsdp` field is passed).\n",
            " |                   If `\"True\"`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\n",
            " |                   all-gathers.\n",
            " |              - use_orig_params (`bool`, *optional*, defaults to `True`)\n",
            " |                  If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n",
            " |                  frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\n",
            " |                  refer this\n",
            " |                  [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n",
            " |              - sync_module_states (`bool`, *optional*, defaults to `True`)\n",
            " |                  If `\"True\"`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\n",
            " |                  ensure they are the same across all ranks after initialization\n",
            " |              - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)\n",
            " |                  If `\"True\"`, only the first process loads the pretrained model checkpoint while all other processes\n",
            " |                  have empty weights.  When this setting as `\"True\"`, `sync_module_states` also must to be `\"True\"`,\n",
            " |                  otherwise all the processes except the main process would have random weights leading to unexpected\n",
            " |                  behaviour during training.\n",
            " |              - activation_checkpointing (`bool`, *optional*, defaults to `False`):\n",
            " |                  If `\"True\"`, activation checkpointing is a technique to reduce memory usage by clearing activations of\n",
            " |                  certain layers and recomputing them during a backward pass. Effectively, this trades extra\n",
            " |                  computation time for reduced memory usage.\n",
            " |              - xla (`bool`, *optional*, defaults to `False`):\n",
            " |                  Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\n",
            " |                  and its API may evolve in the future.\n",
            " |              - xla_fsdp_settings (`dict`, *optional*)\n",
            " |                  The value is a dictionary which stores the XLA FSDP wrapping parameters.\n",
            " |  \n",
            " |                  For a complete list of options, please see [here](\n",
            " |                  https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).\n",
            " |              - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):\n",
            " |                  Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n",
            " |                  used when the xla flag is set to true, and an auto wrapping policy is specified through\n",
            " |                  fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n",
            " |      deepspeed (`str` or `dict`, *optional*):\n",
            " |          Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n",
            " |          evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
            " |          `ds_config.json`) or an already loaded json file as a `dict`\"\n",
            " |  \n",
            " |          <Tip warning={true}>\n",
            " |              If enabling any Zero-init, make sure that your model is not initialized until\n",
            " |              *after* initializing the `TrainingArguments`, else it will not be applied.\n",
            " |          </Tip>\n",
            " |  \n",
            " |      accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):\n",
            " |          Config to be used with the internal `Accelerator` implementation. The value is either a location of\n",
            " |          accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,\n",
            " |          or an instance of [`~trainer_pt_utils.AcceleratorConfig`].\n",
            " |  \n",
            " |          A list of config and its options:\n",
            " |              - split_batches (`bool`, *optional*, defaults to `False`):\n",
            " |                  Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n",
            " |                  `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a\n",
            " |                  round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set\n",
            " |                  in your script multiplied by the number of processes.\n",
            " |              - dispatch_batches (`bool`, *optional*):\n",
            " |                  If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\n",
            " |                  and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose\n",
            " |                  underlying dataset is an `IterableDataset`, `False` otherwise.\n",
            " |              - even_batches (`bool`, *optional*, defaults to `True`):\n",
            " |                  If set to `True`, in cases where the total batch size across all processes does not exactly divide the\n",
            " |                  dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\n",
            " |                  all workers.\n",
            " |              - use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n",
            " |                  Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n",
            " |                  training results are fully reproducible using a different sampling technique. While seed-to-seed results\n",
            " |                  may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n",
            " |                  also be ran with [`~utils.set_seed`] for the best results.\n",
            " |              - use_configured_state (`bool`, *optional*, defaults to `False`):\n",
            " |                  Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n",
            " |                  If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues\n",
            " |                  with hyperparameter tuning.\n",
            " |  \n",
            " |      label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n",
            " |          The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
            " |          labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n",
            " |          label_smoothing_factor/num_labels` respectively.\n",
            " |      debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n",
            " |          Enable one or more debug features. This is an experimental feature.\n",
            " |  \n",
            " |          Possible options are:\n",
            " |  \n",
            " |          - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n",
            " |            the event\n",
            " |          - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
            " |  \n",
            " |          The options should be separated by whitespaces.\n",
            " |      optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n",
            " |          The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n",
            " |          \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n",
            " |          for a full list of optimizers.\n",
            " |      optim_args (`str`, *optional*):\n",
            " |          Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n",
            " |      group_by_length (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
            " |          padding applied and be more efficient). Only useful if applying dynamic padding.\n",
            " |      length_column_name (`str`, *optional*, defaults to `\"length\"`):\n",
            " |          Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
            " |          than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n",
            " |          instance of `Dataset`.\n",
            " |      report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n",
            " |          The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
            " |          `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n",
            " |          `\"swanlab\"`, `\"tensorboard\"`, and `\"wandb\"`. Use `\"all\"` to report to all integrations installed, `\"none\"`\n",
            " |          for no integrations.\n",
            " |      ddp_find_unused_parameters (`bool`, *optional*):\n",
            " |          When using distributed training, the value of the flag `find_unused_parameters` passed to\n",
            " |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
            " |      ddp_bucket_cap_mb (`int`, *optional*):\n",
            " |          When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
            " |      ddp_broadcast_buffers (`bool`, *optional*):\n",
            " |          When using distributed training, the value of the flag `broadcast_buffers` passed to\n",
            " |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
            " |      dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
            " |      dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):\n",
            " |          If True, the data loader will not shut down the worker processes after a dataset has been consumed once.\n",
            " |          This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\n",
            " |          increase RAM usage. Will default to `False`.\n",
            " |      dataloader_prefetch_factor (`int`, *optional*):\n",
            " |          Number of batches loaded in advance by each worker.\n",
            " |          2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
            " |      skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
            " |          down the training and evaluation speed.\n",
            " |      push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n",
            " |          `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n",
            " |          will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n",
            " |          [`~Trainer.save_model`] will also trigger a push.\n",
            " |  \n",
            " |          <Tip warning={true}>\n",
            " |  \n",
            " |          If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n",
            " |          pushed.\n",
            " |  \n",
            " |          </Tip>\n",
            " |  \n",
            " |      resume_from_checkpoint (`str`, *optional*):\n",
            " |          The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
            " |          [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      hub_model_id (`str`, *optional*):\n",
            " |          The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
            " |          which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
            " |          for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
            " |          `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n",
            " |          name of `output_dir`.\n",
            " |  \n",
            " |          Will default to the name of `output_dir`.\n",
            " |      hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
            " |          Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
            " |  \n",
            " |          - `\"end\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
            " |            draft of a model card when the [`~Trainer.save_model`] method is called.\n",
            " |          - `\"every_save\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and\n",
            " |            a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
            " |            training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
            " |            finished. A last push is made with the final model at the end of training.\n",
            " |          - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
            " |            last-checkpoint, allowing you to resume training easily with\n",
            " |            `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
            " |          - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n",
            " |            folder (so you will get one checkpoint folder per folder in your final repository)\n",
            " |  \n",
            " |      hub_token (`str`, *optional*):\n",
            " |          The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
            " |          `huggingface-cli login`.\n",
            " |      hub_private_repo (`bool`, *optional*):\n",
            " |          Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
            " |      hub_always_push (`bool`, *optional*, defaults to `False`):\n",
            " |          Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n",
            " |      gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
            " |          If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
            " |      gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n",
            " |          Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n",
            " |      include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n",
            " |          This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n",
            " |      include_for_metrics (`List[str]`, *optional*, defaults to `[]`):\n",
            " |          Include additional data in the `compute_metrics` function if needed for metrics computation.\n",
            " |          Possible options to add to `include_for_metrics` list:\n",
            " |          - `\"inputs\"`: Input data passed to the model, intended for calculating input dependent metrics.\n",
            " |          - `\"loss\"`: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n",
            " |      eval_do_concat_batches (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,\n",
            " |          will instead store them as lists, with each batch kept separate.\n",
            " |      auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
            " |          Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n",
            " |          CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
            " |      full_determinism (`bool`, *optional*, defaults to `False`)\n",
            " |          If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n",
            " |          distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n",
            " |      torchdynamo (`str`, *optional*):\n",
            " |          If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n",
            " |          `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n",
            " |      ray_scope (`str`, *optional*, defaults to `\"last\"`):\n",
            " |          The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n",
            " |          then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n",
            " |          are also available. See the [Ray documentation](\n",
            " |          https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n",
            " |          more options.\n",
            " |      ddp_timeout (`int`, *optional*, defaults to 1800):\n",
            " |          The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n",
            " |          performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n",
            " |          (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n",
            " |          information.\n",
            " |      use_mps_device (`bool`, *optional*, defaults to `False`):\n",
            " |          This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n",
            " |      torch_compile (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to compile the model using PyTorch 2.0\n",
            " |          [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n",
            " |  \n",
            " |          This will use the best defaults for the [`torch.compile`\n",
            " |          API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).\n",
            " |          You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we\n",
            " |          don't guarantee any of them will work as the support is progressively rolled in in PyTorch.\n",
            " |  \n",
            " |          This flag and the whole compile API is experimental and subject to change in future releases.\n",
            " |      torch_compile_backend (`str`, *optional*):\n",
            " |          The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
            " |  \n",
            " |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
            " |  \n",
            " |          This flag is experimental and subject to change in future releases.\n",
            " |      torch_compile_mode (`str`, *optional*):\n",
            " |          The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
            " |  \n",
            " |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
            " |  \n",
            " |          This flag is experimental and subject to change in future releases.\n",
            " |      include_tokens_per_second (`bool`, *optional*):\n",
            " |          Whether or not to compute the number of tokens per second per device for training speed metrics.\n",
            " |  \n",
            " |          This will iterate over the entire training dataloader once beforehand,\n",
            " |  \n",
            " |          and will slow down the entire process.\n",
            " |  \n",
            " |      include_num_input_tokens_seen (`bool`, *optional*):\n",
            " |          Whether or not to track the number of input tokens seen throughout training.\n",
            " |  \n",
            " |          May be slower in distributed training as gather operations must be called.\n",
            " |  \n",
            " |      neftune_noise_alpha (`Optional[float]`):\n",
            " |          If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance\n",
            " |          for instruction fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914) and the\n",
            " |          [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n",
            " |          `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].\n",
            " |      optim_target_modules (`Union[str, List[str]]`, *optional*):\n",
            " |          The target modules to optimize, i.e. the module names that you would like to train.\n",
            " |          Currently used for the GaLore algorithm (https://arxiv.org/abs/2403.03507) and APOLLO algorithm (https://arxiv.org/abs/2412.05270).\n",
            " |          See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.\n",
            " |          You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \"apollo_adamw\", \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules only.\n",
            " |  \n",
            " |      batch_eval_metrics (`Optional[bool]`, defaults to `False`):\n",
            " |          If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics\n",
            " |          rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function\n",
            " |          that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global\n",
            " |          summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.\n",
            " |  \n",
            " |      eval_on_start (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n",
            " |  \n",
            " |      eval_use_gather_object (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n",
            " |  \n",
            " |      use_liger_kernel (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.\n",
            " |          It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\n",
            " |          flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n",
            " |  \n",
            " |      average_tokens_across_devices (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n",
            " |          num_tokens_in_batch for precise loss calculation. Reference:\n",
            " |          https://github.com/huggingface/transformers/issues/34242\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __eq__(self, other)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __init__(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict, str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict, str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __post_init__(self)\n",
            " |  \n",
            " |  __repr__ = __str__(self)\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  get_process_log_level(self)\n",
            " |      Returns the log level to be used depending on whether this process is the main process of node 0, main process\n",
            " |      of node non-0, or a non-main process.\n",
            " |      \n",
            " |      For the main process the log level defaults to the logging level set (`logging.WARNING` if you didn't do\n",
            " |      anything) unless overridden by `log_level` argument.\n",
            " |      \n",
            " |      For the replica processes the log level defaults to `logging.WARNING` unless overridden by `log_level_replica`\n",
            " |      argument.\n",
            " |      \n",
            " |      The choice between the main and replica process settings is made according to the return value of `should_log`.\n",
            " |  \n",
            " |  get_warmup_steps(self, num_training_steps: int)\n",
            " |      Get number of steps used for a linear warmup.\n",
            " |  \n",
            " |  main_process_first(self, local=True, desc='work')\n",
            " |      A context manager for torch distributed environment where on needs to do something on the main process, while\n",
            " |      blocking replicas, and when it's finished releasing the replicas.\n",
            " |      \n",
            " |      One such use is for `datasets`'s `map` feature which to be efficient should be run once on the main process,\n",
            " |      which upon completion saves a cached version of results and which then automatically gets loaded by the\n",
            " |      replicas.\n",
            " |      \n",
            " |      Args:\n",
            " |          local (`bool`, *optional*, defaults to `True`):\n",
            " |              if `True` first means process of rank 0 of each node if `False` first means process of rank 0 of node\n",
            " |              rank 0 In multi-node environment with a shared filesystem you most likely will want to use\n",
            " |              `local=False` so that only the main process of the first node will do the processing. If however, the\n",
            " |              filesystem is not shared, then the main process of each node will need to do the processing, which is\n",
            " |              the default behavior.\n",
            " |          desc (`str`, *optional*, defaults to `\"work\"`):\n",
            " |              a work description to be used in debug logs\n",
            " |  \n",
            " |  set_dataloader(self, train_batch_size: int = 8, eval_batch_size: int = 8, drop_last: bool = False, num_workers: int = 0, pin_memory: bool = True, persistent_workers: bool = False, prefetch_factor: Optional[int] = None, auto_find_batch_size: bool = False, ignore_data_skip: bool = False, sampler_seed: Optional[int] = None)\n",
            " |      A method that regroups all arguments linked to the dataloaders creation.\n",
            " |      \n",
            " |      Args:\n",
            " |          drop_last (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch\n",
            " |              size) or not.\n",
            " |          num_workers (`int`, *optional*, defaults to 0):\n",
            " |              Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in\n",
            " |              the main process.\n",
            " |          pin_memory (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
            " |          persistent_workers (`bool`, *optional*, defaults to `False`):\n",
            " |              If True, the data loader will not shut down the worker processes after a dataset has been consumed\n",
            " |              once. This allows to maintain the workers Dataset instances alive. Can potentially speed up training,\n",
            " |              but will increase RAM usage. Will default to `False`.\n",
            " |          prefetch_factor (`int`, *optional*):\n",
            " |              Number of batches loaded in advance by each worker.\n",
            " |              2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
            " |          auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
            " |              Whether to find a batch size that will fit into memory automatically through exponential decay,\n",
            " |              avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
            " |          ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
            " |              When resuming training, whether or not to skip the epochs and batches to get the data loading at the\n",
            " |              same stage as in the previous training. If set to `True`, the training will begin faster (as that\n",
            " |              skipping step can take a long time) but will not yield the same results as the interrupted training\n",
            " |              would have.\n",
            " |          sampler_seed (`int`, *optional*):\n",
            " |              Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
            " |              same seed as `self.seed`. This can be used to ensure reproducibility of data sampling, independent of\n",
            " |              the model seed.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)\n",
            " |      >>> args.per_device_train_batch_size\n",
            " |      16\n",
            " |      ```\n",
            " |  \n",
            " |  set_evaluate(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'no', steps: int = 500, batch_size: int = 8, accumulation_steps: Optional[int] = None, delay: Optional[float] = None, loss_only: bool = False, jit_mode: bool = False)\n",
            " |      A method that regroups all arguments linked to evaluation.\n",
            " |      \n",
            " |      Args:\n",
            " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
            " |              The evaluation strategy to adopt during training. Possible values are:\n",
            " |      \n",
            " |                  - `\"no\"`: No evaluation is done during training.\n",
            " |                  - `\"steps\"`: Evaluation is done (and logged) every `steps`.\n",
            " |                  - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
            " |      \n",
            " |              Setting a `strategy` different from `\"no\"` will set `self.do_eval` to `True`.\n",
            " |          steps (`int`, *optional*, defaults to 500):\n",
            " |              Number of update steps between two evaluations if `strategy=\"steps\"`.\n",
            " |          batch_size (`int` *optional*, defaults to 8):\n",
            " |              The batch size per device (GPU/TPU core/CPU...) used for evaluation.\n",
            " |          accumulation_steps (`int`, *optional*):\n",
            " |              Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU.\n",
            " |              If left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster\n",
            " |              but requires more memory).\n",
            " |          delay (`float`, *optional*):\n",
            " |              Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
            " |              eval_strategy.\n",
            " |          loss_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Ignores all outputs except the loss.\n",
            " |          jit_mode (`bool`, *optional*):\n",
            " |              Whether or not to use PyTorch jit trace for inference.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_evaluate(strategy=\"steps\", steps=100)\n",
            " |      >>> args.eval_steps\n",
            " |      100\n",
            " |      ```\n",
            " |  \n",
            " |  set_logging(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps', steps: int = 500, report_to: Union[str, list[str]] = 'none', level: str = 'passive', first_step: bool = False, nan_inf_filter: bool = False, on_each_node: bool = False, replica_level: str = 'passive')\n",
            " |      A method that regroups all arguments linked to logging.\n",
            " |      \n",
            " |      Args:\n",
            " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |              The logging strategy to adopt during training. Possible values are:\n",
            " |      \n",
            " |                  - `\"no\"`: No logging is done during training.\n",
            " |                  - `\"epoch\"`: Logging is done at the end of each epoch.\n",
            " |                  - `\"steps\"`: Logging is done every `logging_steps`.\n",
            " |      \n",
            " |          steps (`int`, *optional*, defaults to 500):\n",
            " |              Number of update steps between two logs if `strategy=\"steps\"`.\n",
            " |          level (`str`, *optional*, defaults to `\"passive\"`):\n",
            " |              Logger log level to use on the main process. Possible choices are the log levels as strings: `\"debug\"`,\n",
            " |              `\"info\"`, `\"warning\"`, `\"error\"` and `\"critical\"`, plus a `\"passive\"` level which doesn't set anything\n",
            " |              and lets the application set the level.\n",
            " |          report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n",
            " |              The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
            " |              `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`,\n",
            " |              `\"neptune\"`, `\"swanlab\"`, `\"tensorboard\"`, and `\"wandb\"`. Use `\"all\"` to report to all integrations\n",
            " |              installed, `\"none\"` for no integrations.\n",
            " |          first_step (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to log and evaluate the first `global_step` or not.\n",
            " |          nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is\n",
            " |              `nan` or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
            " |      \n",
            " |              <Tip>\n",
            " |      \n",
            " |              `nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
            " |              gradient is computed or applied to the model.\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |          on_each_node (`bool`, *optional*, defaults to `True`):\n",
            " |              In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
            " |              node.\n",
            " |          replica_level (`str`, *optional*, defaults to `\"passive\"`):\n",
            " |              Logger log level to use on replicas. Same choices as `log_level`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_logging(strategy=\"steps\", steps=100)\n",
            " |      >>> args.logging_steps\n",
            " |      100\n",
            " |      ```\n",
            " |  \n",
            " |  set_lr_scheduler(self, name: Union[str, transformers.trainer_utils.SchedulerType] = 'linear', num_epochs: float = 3.0, max_steps: int = -1, warmup_ratio: float = 0, warmup_steps: int = 0)\n",
            " |      A method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
            " |              The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
            " |          num_epochs(`float`, *optional*, defaults to 3.0):\n",
            " |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n",
            " |              of the last epoch before stopping training).\n",
            " |          max_steps (`int`, *optional*, defaults to -1):\n",
            " |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
            " |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
            " |              `max_steps` is reached.\n",
            " |          warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
            " |              Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
            " |          warmup_steps (`int`, *optional*, defaults to 0):\n",
            " |              Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of\n",
            " |              `warmup_ratio`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n",
            " |      >>> args.warmup_ratio\n",
            " |      0.05\n",
            " |      ```\n",
            " |  \n",
            " |  set_optimizer(self, name: Union[str, transformers.training_args.OptimizerNames] = 'adamw_torch', learning_rate: float = 5e-05, weight_decay: float = 0, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-08, args: Optional[str] = None)\n",
            " |      A method that regroups all arguments linked to the optimizer and its hyperparameters.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n",
            " |              The optimizer to use: `\"adamw_torch\"`, `\"adamw_torch_fused\"`, `\"adamw_apex_fused\"`,\n",
            " |              `\"adamw_anyprecision\"` or `\"adafactor\"`.\n",
            " |          learning_rate (`float`, *optional*, defaults to 5e-5):\n",
            " |              The initial learning rate.\n",
            " |          weight_decay (`float`, *optional*, defaults to 0):\n",
            " |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights.\n",
            " |          beta1 (`float`, *optional*, defaults to 0.9):\n",
            " |              The beta1 hyperparameter for the adam optimizer or its variants.\n",
            " |          beta2 (`float`, *optional*, defaults to 0.999):\n",
            " |              The beta2 hyperparameter for the adam optimizer or its variants.\n",
            " |          epsilon (`float`, *optional*, defaults to 1e-8):\n",
            " |              The epsilon hyperparameter for the adam optimizer or its variants.\n",
            " |          args (`str`, *optional*):\n",
            " |              Optional arguments that are supplied to AnyPrecisionAdamW (only useful when\n",
            " |              `optim=\"adamw_anyprecision\"`).\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n",
            " |      >>> args.optim\n",
            " |      'adamw_torch'\n",
            " |      ```\n",
            " |  \n",
            " |  set_push_to_hub(self, model_id: str, strategy: Union[str, transformers.trainer_utils.HubStrategy] = 'every_save', token: Optional[str] = None, private_repo: Optional[bool] = None, always_push: bool = False)\n",
            " |      A method that regroups all arguments linked to synchronizing checkpoints with the Hub.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      Calling this method will set `self.push_to_hub` to `True`, which means the `output_dir` will begin a git\n",
            " |      directory synced with the repo (determined by `model_id`) and the content will be pushed each time a save is\n",
            " |      triggered (depending on your `self.save_strategy`). Calling [`~Trainer.save_model`] will also trigger a push.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          model_id (`str`):\n",
            " |              The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
            " |              which case the model will be pushed in your namespace. Otherwise it should be the whole repository\n",
            " |              name, for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of\n",
            " |              with `\"organization_name/model\"`.\n",
            " |          strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
            " |              Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
            " |      \n",
            " |              - `\"end\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
            " |              draft of a model card when the [`~Trainer.save_model`] method is called.\n",
            " |              - `\"every_save\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`])\n",
            " |                and\n",
            " |              a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
            " |              training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
            " |              finished. A last push is made with the final model at the end of training.\n",
            " |              - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
            " |              last-checkpoint, allowing you to resume training easily with\n",
            " |              `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
            " |              - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the\n",
            " |                output\n",
            " |              folder (so you will get one checkpoint folder per folder in your final repository)\n",
            " |      \n",
            " |          token (`str`, *optional*):\n",
            " |              The token to use to push the model to the Hub. Will default to the token in the cache folder obtained\n",
            " |              with `huggingface-cli login`.\n",
            " |          private_repo (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
            " |          always_push (`bool`, *optional*, defaults to `False`):\n",
            " |              Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not\n",
            " |              finished.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_push_to_hub(\"me/awesome-model\")\n",
            " |      >>> args.hub_model_id\n",
            " |      'me/awesome-model'\n",
            " |      ```\n",
            " |  \n",
            " |  set_save(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps', steps: int = 500, total_limit: Optional[int] = None, on_each_node: bool = False)\n",
            " |      A method that regroups all arguments linked to checkpoint saving.\n",
            " |      \n",
            " |      Args:\n",
            " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |              The checkpoint save strategy to adopt during training. Possible values are:\n",
            " |      \n",
            " |                  - `\"no\"`: No save is done during training.\n",
            " |                  - `\"epoch\"`: Save is done at the end of each epoch.\n",
            " |                  - `\"steps\"`: Save is done every `save_steps`.\n",
            " |      \n",
            " |          steps (`int`, *optional*, defaults to 500):\n",
            " |              Number of updates steps before two checkpoint saves if `strategy=\"steps\"`.\n",
            " |          total_limit (`int`, *optional*):\n",
            " |              If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
            " |              `output_dir`.\n",
            " |          on_each_node (`bool`, *optional*, defaults to `False`):\n",
            " |              When doing multi-node distributed training, whether to save models and checkpoints on each node, or\n",
            " |              only on the main one.\n",
            " |      \n",
            " |              This should not be activated when the different nodes use the same storage as the files will be saved\n",
            " |              with the same names for each node.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_save(strategy=\"steps\", steps=100)\n",
            " |      >>> args.save_steps\n",
            " |      100\n",
            " |      ```\n",
            " |  \n",
            " |  set_testing(self, batch_size: int = 8, loss_only: bool = False, jit_mode: bool = False)\n",
            " |      A method that regroups all basic arguments linked to testing on a held-out dataset.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      Calling this method will automatically set `self.do_predict` to `True`.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          batch_size (`int` *optional*, defaults to 8):\n",
            " |              The batch size per device (GPU/TPU core/CPU...) used for testing.\n",
            " |          loss_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Ignores all outputs except the loss.\n",
            " |          jit_mode (`bool`, *optional*):\n",
            " |              Whether or not to use PyTorch jit trace for inference.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_testing(batch_size=32)\n",
            " |      >>> args.per_device_eval_batch_size\n",
            " |      32\n",
            " |      ```\n",
            " |  \n",
            " |  set_training(self, learning_rate: float = 5e-05, batch_size: int = 8, weight_decay: float = 0, num_epochs: float = 3, max_steps: int = -1, gradient_accumulation_steps: int = 1, seed: int = 42, gradient_checkpointing: bool = False)\n",
            " |      A method that regroups all basic arguments linked to the training.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      Calling this method will automatically set `self.do_train` to `True`.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          learning_rate (`float`, *optional*, defaults to 5e-5):\n",
            " |              The initial learning rate for the optimizer.\n",
            " |          batch_size (`int` *optional*, defaults to 8):\n",
            " |              The batch size per device (GPU/TPU core/CPU...) used for training.\n",
            " |          weight_decay (`float`, *optional*, defaults to 0):\n",
            " |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in the\n",
            " |              optimizer.\n",
            " |          num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
            " |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n",
            " |              of the last epoch before stopping training).\n",
            " |          max_steps (`int`, *optional*, defaults to -1):\n",
            " |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
            " |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
            " |              `max_steps` is reached.\n",
            " |          gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
            " |              Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
            " |      \n",
            " |              <Tip warning={true}>\n",
            " |      \n",
            " |              When using gradient accumulation, one step is counted as one step with backward pass. Therefore,\n",
            " |              logging, evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training\n",
            " |              examples.\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |          seed (`int`, *optional*, defaults to 42):\n",
            " |              Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use\n",
            " |              the [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized\n",
            " |              parameters.\n",
            " |          gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
            " |              If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |      \n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_training(learning_rate=1e-4, batch_size=32)\n",
            " |      >>> args.learning_rate\n",
            " |      1e-4\n",
            " |      ```\n",
            " |  \n",
            " |  to_dict(self)\n",
            " |      Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
            " |      the token values by removing their value.\n",
            " |  \n",
            " |  to_json_string(self)\n",
            " |      Serializes this instance to a JSON string.\n",
            " |  \n",
            " |  to_sanitized_dict(self) -> dict[str, typing.Any]\n",
            " |      Sanitized serialization to use with TensorBoard’s hparams\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  ddp_timeout_delta\n",
            " |      The actual timeout for torch.distributed.init_process_group since it expects a timedelta variable.\n",
            " |  \n",
            " |  device\n",
            " |      The device used by this process.\n",
            " |  \n",
            " |  eval_batch_size\n",
            " |      The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).\n",
            " |  \n",
            " |  local_process_index\n",
            " |      The index of the local process used.\n",
            " |  \n",
            " |  n_gpu\n",
            " |      The number of GPUs used by this process.\n",
            " |      \n",
            " |      Note:\n",
            " |          This will only be greater than one when you have multiple GPUs available but are not using distributed\n",
            " |          training. For distributed training, it will always be 1.\n",
            " |  \n",
            " |  parallel_mode\n",
            " |      The current mode used for parallelism if multiple GPUs/TPU cores are available. One of:\n",
            " |      \n",
            " |      - `ParallelMode.NOT_PARALLEL`: no parallelism (CPU or one GPU).\n",
            " |      - `ParallelMode.NOT_DISTRIBUTED`: several GPUs in one single process (uses `torch.nn.DataParallel`).\n",
            " |      - `ParallelMode.DISTRIBUTED`: several GPUs, each having its own process (uses\n",
            " |        `torch.nn.DistributedDataParallel`).\n",
            " |      - `ParallelMode.TPU`: several TPU cores.\n",
            " |  \n",
            " |  place_model_on_device\n",
            " |      Can be subclassed and overridden for some specific integrations.\n",
            " |  \n",
            " |  process_index\n",
            " |      The index of the current process used.\n",
            " |  \n",
            " |  should_log\n",
            " |      Whether or not the current process should produce log.\n",
            " |  \n",
            " |  should_save\n",
            " |      Whether or not the current process should write to disk, e.g., to save models and checkpoints.\n",
            " |  \n",
            " |  train_batch_size\n",
            " |      The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).\n",
            " |  \n",
            " |  world_size\n",
            " |      The number of processes used in parallel.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_n_gpu': <class 'int'>, 'accelerator_config': typi...\n",
            " |  \n",
            " |  __dataclass_fields__ = {'_n_gpu': Field(name='_n_gpu',type=<class 'int...\n",
            " |  \n",
            " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __match_args__ = ('output_dir', 'overwrite_output_dir', 'do_train', 'd...\n",
            " |  \n",
            " |  accelerator_config = None\n",
            " |  \n",
            " |  adafactor = False\n",
            " |  \n",
            " |  adam_beta1 = 0.9\n",
            " |  \n",
            " |  adam_beta2 = 0.999\n",
            " |  \n",
            " |  adam_epsilon = 1e-08\n",
            " |  \n",
            " |  auto_find_batch_size = False\n",
            " |  \n",
            " |  average_tokens_across_devices = False\n",
            " |  \n",
            " |  batch_eval_metrics = False\n",
            " |  \n",
            " |  bf16 = False\n",
            " |  \n",
            " |  bf16_full_eval = False\n",
            " |  \n",
            " |  data_seed = None\n",
            " |  \n",
            " |  dataloader_drop_last = False\n",
            " |  \n",
            " |  dataloader_num_workers = 0\n",
            " |  \n",
            " |  dataloader_persistent_workers = False\n",
            " |  \n",
            " |  dataloader_pin_memory = True\n",
            " |  \n",
            " |  dataloader_prefetch_factor = None\n",
            " |  \n",
            " |  ddp_backend = None\n",
            " |  \n",
            " |  ddp_broadcast_buffers = None\n",
            " |  \n",
            " |  ddp_bucket_cap_mb = None\n",
            " |  \n",
            " |  ddp_find_unused_parameters = None\n",
            " |  \n",
            " |  ddp_timeout = 1800\n",
            " |  \n",
            " |  debug = ''\n",
            " |  \n",
            " |  deepspeed = None\n",
            " |  \n",
            " |  default_optim = 'adamw_torch'\n",
            " |  \n",
            " |  disable_tqdm = None\n",
            " |  \n",
            " |  do_eval = False\n",
            " |  \n",
            " |  do_predict = False\n",
            " |  \n",
            " |  do_train = False\n",
            " |  \n",
            " |  eval_accumulation_steps = None\n",
            " |  \n",
            " |  eval_delay = 0\n",
            " |  \n",
            " |  eval_do_concat_batches = True\n",
            " |  \n",
            " |  eval_on_start = False\n",
            " |  \n",
            " |  eval_steps = None\n",
            " |  \n",
            " |  eval_strategy = 'no'\n",
            " |  \n",
            " |  eval_use_gather_object = False\n",
            " |  \n",
            " |  fp16 = False\n",
            " |  \n",
            " |  fp16_backend = 'auto'\n",
            " |  \n",
            " |  fp16_full_eval = False\n",
            " |  \n",
            " |  fp16_opt_level = 'O1'\n",
            " |  \n",
            " |  framework = 'pt'\n",
            " |  \n",
            " |  fsdp = ''\n",
            " |  \n",
            " |  fsdp_config = None\n",
            " |  \n",
            " |  fsdp_min_num_params = 0\n",
            " |  \n",
            " |  fsdp_transformer_layer_cls_to_wrap = None\n",
            " |  \n",
            " |  full_determinism = False\n",
            " |  \n",
            " |  gradient_accumulation_steps = 1\n",
            " |  \n",
            " |  gradient_checkpointing = False\n",
            " |  \n",
            " |  gradient_checkpointing_kwargs = None\n",
            " |  \n",
            " |  greater_is_better = None\n",
            " |  \n",
            " |  group_by_length = False\n",
            " |  \n",
            " |  half_precision_backend = 'auto'\n",
            " |  \n",
            " |  hub_always_push = False\n",
            " |  \n",
            " |  hub_model_id = None\n",
            " |  \n",
            " |  hub_private_repo = None\n",
            " |  \n",
            " |  hub_strategy = 'every_save'\n",
            " |  \n",
            " |  hub_token = None\n",
            " |  \n",
            " |  ignore_data_skip = False\n",
            " |  \n",
            " |  include_inputs_for_metrics = False\n",
            " |  \n",
            " |  include_num_input_tokens_seen = False\n",
            " |  \n",
            " |  include_tokens_per_second = False\n",
            " |  \n",
            " |  jit_mode_eval = False\n",
            " |  \n",
            " |  label_names = None\n",
            " |  \n",
            " |  label_smoothing_factor = 0.0\n",
            " |  \n",
            " |  learning_rate = 5e-05\n",
            " |  \n",
            " |  length_column_name = 'length'\n",
            " |  \n",
            " |  load_best_model_at_end = False\n",
            " |  \n",
            " |  local_rank = -1\n",
            " |  \n",
            " |  log_level = 'passive'\n",
            " |  \n",
            " |  log_level_replica = 'warning'\n",
            " |  \n",
            " |  log_on_each_node = True\n",
            " |  \n",
            " |  logging_dir = None\n",
            " |  \n",
            " |  logging_first_step = False\n",
            " |  \n",
            " |  logging_nan_inf_filter = True\n",
            " |  \n",
            " |  logging_steps = 500\n",
            " |  \n",
            " |  logging_strategy = 'steps'\n",
            " |  \n",
            " |  lr_scheduler_type = 'linear'\n",
            " |  \n",
            " |  max_grad_norm = 1.0\n",
            " |  \n",
            " |  max_steps = -1\n",
            " |  \n",
            " |  metric_for_best_model = None\n",
            " |  \n",
            " |  mp_parameters = ''\n",
            " |  \n",
            " |  neftune_noise_alpha = None\n",
            " |  \n",
            " |  no_cuda = False\n",
            " |  \n",
            " |  num_train_epochs = 3.0\n",
            " |  \n",
            " |  optim = 'adamw_torch'\n",
            " |  \n",
            " |  optim_args = None\n",
            " |  \n",
            " |  optim_target_modules = None\n",
            " |  \n",
            " |  output_dir = None\n",
            " |  \n",
            " |  overwrite_output_dir = False\n",
            " |  \n",
            " |  past_index = -1\n",
            " |  \n",
            " |  per_device_eval_batch_size = 8\n",
            " |  \n",
            " |  per_device_train_batch_size = 8\n",
            " |  \n",
            " |  per_gpu_eval_batch_size = None\n",
            " |  \n",
            " |  per_gpu_train_batch_size = None\n",
            " |  \n",
            " |  prediction_loss_only = False\n",
            " |  \n",
            " |  push_to_hub = False\n",
            " |  \n",
            " |  push_to_hub_model_id = None\n",
            " |  \n",
            " |  push_to_hub_organization = None\n",
            " |  \n",
            " |  push_to_hub_token = None\n",
            " |  \n",
            " |  ray_scope = 'last'\n",
            " |  \n",
            " |  remove_unused_columns = True\n",
            " |  \n",
            " |  report_to = None\n",
            " |  \n",
            " |  restore_callback_states_from_checkpoint = False\n",
            " |  \n",
            " |  resume_from_checkpoint = None\n",
            " |  \n",
            " |  run_name = None\n",
            " |  \n",
            " |  save_on_each_node = False\n",
            " |  \n",
            " |  save_only_model = False\n",
            " |  \n",
            " |  save_safetensors = True\n",
            " |  \n",
            " |  save_steps = 500\n",
            " |  \n",
            " |  save_strategy = 'steps'\n",
            " |  \n",
            " |  save_total_limit = None\n",
            " |  \n",
            " |  seed = 42\n",
            " |  \n",
            " |  skip_memory_metrics = True\n",
            " |  \n",
            " |  tf32 = None\n",
            " |  \n",
            " |  torch_compile = False\n",
            " |  \n",
            " |  torch_compile_backend = None\n",
            " |  \n",
            " |  torch_compile_mode = None\n",
            " |  \n",
            " |  torch_empty_cache_steps = None\n",
            " |  \n",
            " |  torchdynamo = None\n",
            " |  \n",
            " |  tpu_metrics_debug = False\n",
            " |  \n",
            " |  tpu_num_cores = None\n",
            " |  \n",
            " |  use_cpu = False\n",
            " |  \n",
            " |  use_ipex = False\n",
            " |  \n",
            " |  use_legacy_prediction_loop = False\n",
            " |  \n",
            " |  use_liger_kernel = False\n",
            " |  \n",
            " |  use_mps_device = False\n",
            " |  \n",
            " |  warmup_ratio = 0.0\n",
            " |  \n",
            " |  warmup_steps = 0\n",
            " |  \n",
            " |  weight_decay = 0.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}