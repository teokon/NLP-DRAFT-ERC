{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqdPsxQFAWgw"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Teacher PLM Training for BERT-ERC (RoBERTa) with Fine-Grained Classification Head\n",
        "+ Freeze first 8 encoder layers (paper-style)\n",
        "+ Robust teacher predictions export (train/dev with confidences)\n",
        "\n",
        "- Tri-pooling (Fp, Fq, Ff) over token embeddings of:\n",
        "    Fp = tokens before utterance span\n",
        "    Fq = utterance span\n",
        "    Ff = tokens after utterance span\n",
        "- 2-layer MLP head (Tanh + Dropout) -> 7 emotions\n",
        "- Exports: Dialogue_ID, Utterance_ID, Speaker, Utterance, pred_id, pred, conf\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# 1) Setup (Colab-friendly)\n",
        "# =========================\n",
        "!pip -q install transformers datasets evaluate\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, RobertaConfig, Trainer, TrainingArguments,\n",
        "    RobertaPreTrainedModel, RobertaModel\n",
        ")\n",
        "import torch.nn as nn\n",
        "import evaluate\n",
        "\n",
        "# (Optional) Colab Drive mount (safe: only mounts if not already mounted)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.ismount('/content/drive'):\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "    else:\n",
        "        print(\"âœ… Drive already mounted at /content/drive\")\n",
        "except Exception as e:\n",
        "    print(\"â„¹ï¸ Not running on Colab or Drive not available:\", e)\n",
        "\n",
        "# ======================\n",
        "# 2) Configuration\n",
        "# ======================\n",
        "BASE_DIR           = \"/content/drive/MyDrive/MELD\"\n",
        "RAW_TRAIN_CSV      = os.path.join(BASE_DIR, \"train_with_context.csv\")\n",
        "RAW_DEV_CSV        = os.path.join(BASE_DIR, \"dev_with_context.csv\")\n",
        "TEACHER_OUTPUT     = os.path.join(BASE_DIR, \"teacher_roberta_fg_head_marked\")\n",
        "TEACHER_PRED_TRAIN = os.path.join(BASE_DIR, \"teacher_predictions_train.csv\")\n",
        "TEACHER_PRED_DEV   = os.path.join(BASE_DIR, \"teacher_predictions_dev.csv\")\n",
        "\n",
        "MODEL_CHECKPOINT   = \"roberta-base\"\n",
        "EMOTIONS           = [\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\"]\n",
        "NUM_LABELS         = len(EMOTIONS)\n",
        "\n",
        "MAX_LEN            = 128\n",
        "BATCH_SIZE         = 8\n",
        "EPOCHS             = 4\n",
        "LR                 = 9e-5\n",
        "WEIGHT_DECAY       = 0.01\n",
        "DEVICE             = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "os.makedirs(TEACHER_OUTPUT, exist_ok=True)\n",
        "\n",
        "# ======================\n",
        "# 3) Load data\n",
        "# ======================\n",
        "df_train = pd.read_csv(RAW_TRAIN_CSV)\n",
        "df_dev   = pd.read_csv(RAW_DEV_CSV)\n",
        "\n",
        "label2id = {e:i for i,e in enumerate(EMOTIONS)}\n",
        "id2label = {i:e for e,i in label2id.items()}\n",
        "df_train['label'] = df_train['Emotion'].map(label2id)\n",
        "df_dev['label']   = df_dev['Emotion'].map(label2id)\n",
        "\n",
        "# ======================\n",
        "# 4) Tokenizer & Config\n",
        "# ======================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
        "config    = RobertaConfig.from_pretrained(MODEL_CHECKPOINT, num_labels=NUM_LABELS)\n",
        "\n",
        "# ======================\n",
        "# 5) Fine-grained head\n",
        "# ======================\n",
        "class FineGrainedHead(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_labels, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(input_dim, hidden_dim)\n",
        "        self.act  = nn.Tanh()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc2  = nn.Linear(hidden_dim, num_labels)\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "# ===============================================\n",
        "# 6) Teacher model (RoBERTa + tri-pool + FG head)\n",
        "# ===============================================\n",
        "class RobertaTeacherFG(RobertaPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.roberta = RobertaModel(config)\n",
        "        H = config.hidden_size\n",
        "        self.head = FineGrainedHead(\n",
        "            input_dim=3*H,\n",
        "            hidden_dim=H,\n",
        "            num_labels=config.num_labels,\n",
        "            dropout=config.hidden_dropout_prob\n",
        "        )\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, spans=None, labels=None, **kwargs):\n",
        "        out = self.roberta(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        hs = out.last_hidden_state  # [B, S, H]\n",
        "        B, S, H = hs.size()\n",
        "\n",
        "        # tri-pooling\n",
        "        Fp = torch.zeros(B, H, device=hs.device)\n",
        "        Fq = torch.zeros(B, H, device=hs.device)\n",
        "        Ff = torch.zeros(B, H, device=hs.device)\n",
        "\n",
        "        # spans: [B, 2] with (a, b); may be (-1, -1) if not found\n",
        "        for i, (a, b) in enumerate(spans.tolist()):\n",
        "            if a > 0:\n",
        "                Fp[i] = hs[i, :a].mean(0)\n",
        "            if (b >= a) and (a >= 0):\n",
        "                Fq[i] = hs[i, a:b+1].mean(0)\n",
        "            if (b + 1) < S and b >= -1:\n",
        "                Ff[i] = hs[i, b+1:].mean(0)\n",
        "\n",
        "        cat    = torch.cat([Fp, Fq, Ff], dim=1)\n",
        "        logits = self.head(cat)\n",
        "        loss   = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n",
        "# ======================\n",
        "# 7) Freeze first 8 layers\n",
        "# ======================\n",
        "def freeze_first_n_layers_roberta(model, n=8, freeze_embeddings=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Freeze the first `n` transformer blocks on a RoBERTa-based model.\n",
        "    Expects `model.roberta` to be the backbone.\n",
        "    \"\"\"\n",
        "    if not hasattr(model, \"roberta\"):\n",
        "        raise ValueError(\"Expected the backbone at model.roberta\")\n",
        "    backbone = model.roberta\n",
        "\n",
        "    if freeze_embeddings and hasattr(backbone, \"embeddings\"):\n",
        "        for p in backbone.embeddings.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    layers = backbone.encoder.layer\n",
        "    n = min(n, len(layers))\n",
        "    for i in range(n):\n",
        "        for p in layers[i].parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"ðŸ”’ Froze encoder layers [0..{n-1}] / {len(layers)} (freeze_embeddings={freeze_embeddings})\")\n",
        "\n",
        "# ======================\n",
        "# 8) Build tensors\n",
        "# ======================\n",
        "def build_tensors(df):\n",
        "    ids, masks, spans, labels = [], [], [], []\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['bert_input']\n",
        "        enc  = tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        ids.append(enc['input_ids'].squeeze(0))\n",
        "        masks.append(enc['attention_mask'].squeeze(0))\n",
        "\n",
        "        # find utterance span (simple exact sub-token match)\n",
        "        token_ids = enc['input_ids'].squeeze(0).tolist()\n",
        "        utt_enc   = tokenizer(row['Utterance'], add_special_tokens=False)['input_ids']\n",
        "\n",
        "        try:\n",
        "            a = next(i for i in range(len(token_ids) - len(utt_enc) + 1)\n",
        "                     if token_ids[i:i+len(utt_enc)] == utt_enc)\n",
        "            b = a + len(utt_enc) - 1\n",
        "        except StopIteration:\n",
        "            a, b = -1, -1  # not found\n",
        "\n",
        "        spans.append(torch.tensor([a, b], dtype=torch.long))\n",
        "        labels.append(torch.tensor(row['label'], dtype=torch.long))\n",
        "\n",
        "    return (\n",
        "        torch.stack(ids),\n",
        "        torch.stack(masks),\n",
        "        torch.stack(spans),\n",
        "        torch.stack(labels)\n",
        "    )\n",
        "\n",
        "train_ids, train_masks, train_spans, train_labels = build_tensors(df_train)\n",
        "dev_ids,   dev_masks,   dev_spans,   dev_labels   = build_tensors(df_dev)\n",
        "\n",
        "train_ds = TensorDataset(train_ids, train_masks, train_spans, train_labels)\n",
        "dev_ds   = TensorDataset(dev_ids,   dev_masks,   dev_spans,   dev_labels)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids      = torch.stack([x[0] for x in batch])\n",
        "    attention_mask = torch.stack([x[1] for x in batch])\n",
        "    spans          = torch.stack([x[2] for x in batch])\n",
        "    labels         = torch.stack([x[3] for x in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'spans': spans, 'labels': labels}\n",
        "\n",
        "# ======================\n",
        "# 9) Train\n",
        "# ======================\n",
        "model_teacher = RobertaTeacherFG.from_pretrained(MODEL_CHECKPOINT, config=config).to(DEVICE)\n",
        "freeze_first_n_layers_roberta(model_teacher, n=8, freeze_embeddings=False, verbose=True)\n",
        "\n",
        "acc_metric = evaluate.load('accuracy')\n",
        "f1_metric  = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    y_pred = p.predictions.argmax(-1)\n",
        "    y_true = p.label_ids\n",
        "    return {\n",
        "        'acc': acc_metric.compute(predictions=y_pred, references=y_true)['accuracy'],\n",
        "        'f1_weighted': f1_metric.compute(predictions=y_pred, references=y_true, average='weighted')['f1']\n",
        "    }\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=TEACHER_OUTPUT,\n",
        "    eval_strategy='epoch',         # <- correct arg name\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1_weighted',\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=[\"none\"],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_teacher,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=dev_ds,\n",
        "    data_collator=collate_fn,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(TEACHER_OUTPUT)\n",
        "tokenizer.save_pretrained(TEACHER_OUTPUT)\n",
        "\n",
        "print(\"âœ… Training done. Model & tokenizer saved to:\", TEACHER_OUTPUT)\n",
        "\n",
        "# =======================================\n",
        "# 10) Export teacher predictions (robust)\n",
        "# =======================================\n",
        "def _pick(df_src, names):\n",
        "    low = {c.lower(): c for c in df_src.columns}\n",
        "    for n in names:\n",
        "        if n in df_src.columns: return n\n",
        "        if n.lower() in low: return low[n.lower()]\n",
        "    raise KeyError(f\"Need one of {names}, have {list(df_src.columns)}\")\n",
        "\n",
        "def export_teacher_preds(df_src, ids, masks, spans, out_csv):\n",
        "    # keep exact row order\n",
        "    dummy_labels = torch.zeros(len(df_src), dtype=torch.long)\n",
        "    ds = TensorDataset(ids, masks, spans, dummy_labels)\n",
        "\n",
        "    pred_out = trainer.predict(ds)\n",
        "    logits = torch.from_numpy(pred_out.predictions)\n",
        "    probs  = torch.softmax(logits, dim=-1).numpy()\n",
        "    yhat   = probs.argmax(axis=-1)\n",
        "    conf   = probs.max(axis=-1)\n",
        "\n",
        "    C_DID = _pick(df_src, [\"Dialogue_ID\",\"dialogue_id\",\"Conversation_ID\",\"conv_id\"])\n",
        "    C_UID = _pick(df_src, [\"Utterance_ID\",\"utterance_id\",\"Utterance_ID_in_Dialogue\",\"utt_id\"])\n",
        "    C_SPK = _pick(df_src, [\"Speaker\",\"speaker\",\"speaker_id\"])\n",
        "    C_UTT = _pick(df_src, [\"Utterance\",\"utterance\",\"text\"])\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"Dialogue_ID\":  df_src[C_DID].values,\n",
        "        \"Utterance_ID\": df_src[C_UID].values,\n",
        "        \"Speaker\":      df_src[C_SPK].astype(str).values,\n",
        "        \"Utterance\":    df_src[C_UTT].astype(str).values,\n",
        "        \"pred_id\":      yhat,\n",
        "        \"pred\":         [EMOTIONS[i] for i in yhat],\n",
        "        \"conf\":         conf\n",
        "    })\n",
        "    out.to_csv(out_csv, index=False)\n",
        "    print(f\"âœ“ Saved {out_csv} (rows: {len(out)})\")\n",
        "\n",
        "export_teacher_preds(df_train, train_ids, train_masks, train_spans, TEACHER_PRED_TRAIN)\n",
        "export_teacher_preds(df_dev,   dev_ids,   dev_masks,   dev_spans,   TEACHER_PRED_DEV)\n",
        "\n",
        "print(\"âœ… Done. Exports:\")\n",
        "print(\"  -\", TEACHER_PRED_TRAIN)\n",
        "print(\"  -\", TEACHER_PRED_DEV)\n"
      ]
    }
  ]
}